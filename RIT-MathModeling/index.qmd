---
title: "Introduction to Model Calibration and Uncertainty Quantification"
author: "Vivek Srikrishnan"
course: "Math 622"
institution: "RIT"
date: "October 17, 2023"
location: "RIT"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../css/slides.scss
        template-partials:
            - ../title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: ../mathjax-config.html
        date-format: long
        fig-align: center
        pdf-page-height-offset: 1
        pdf-separate-fragments: true
execute:
    freeze: auto
    echo: false
---

```{julia}
import Pkg
Pkg.activate(".")
Pkg.instantiate()
```

```{julia}
using CSVFiles
using DataFrames
using Plots
using Measures
using StatsBase
using Optim
```

# Introduction

## To Follow Along

These slides are available at <https://viveks.me/talks/RIT-MathModeling/>.

## "All Models Are Wrong, But Some Are Useful"

::: {.quote}

> ...all models are approximations. **Essentially, all models are wrong, but some are useful**. However, the approximate nature of the model must always be borne in mind....

::: {.cite}
--- Box & Draper, *Empirical Model Building and Response Surfaces*, 1987
:::
:::

## What Are Models Good For?

::: {.quote}

> Models can corroborate a hypothesis by offering evidence to strengthen what may be already partly established through other means... 
>
>
> **Thus, the primary value of models is heuristic: Models are representations, useful for guiding further study but not susceptible to proof.**

::: {.cite} 
--- Oreskes et al, ["Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences"](https://www.science.org/doi/10.1126/science.263.5147.641), 1994
:::
:::


## Models And Assumptions

![XKCD Comic 2355](https://imgs.xkcd.com/comics/university_covid_model_2x.png)

::: {.caption}
Source: [XKCD 2355](https://xkcd.com/2355)
:::

## Model Calibration, Validation, and Uncertainty Analysis

This implies that it is important to:

::: {.incremental}
- Select appropriate parameter values (**model calibration**);
- Make sure that the calibrated model is capturing key dynamics (**model validation**);
- Account for uncertainty in the parameter estimates and projections (**uncertainty analysis**)
:::

## These Are Not Really Separate Processes

Calibration, validation, and uncertainty analysis are not necessary separate "steps", but often are mixed together in the process of model development and checking.

# Model Calibration

## What is Model Calibration?

**Calibration** involves choosing parameters, ideally based on some principled criteria.


::: {.quote}
> [Calibration is] the selection of model parameters and structures to maximize the fidelity of the system model to observational data given model and computational constraints...

::: {.cite} 
--- Oreskes et al, ["Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences"](https://www.science.org/doi/10.1126/science.263.5147.641), 1994
:::
:::

## Why Does Calibration Matter?

- Models are necessarily simplifications of reality.
- Missing or simplified processes may mean that direct translation of physical equation and parameters into the model do not yield the desired behavior.

## Approaches to Calibration

1. *Hand-Tuning*: Manually adjusting parameters until the model output looks "right".
2. *Statistical Calibration*: Wide category of statistically-based methods to find parameters which are most consistent with data.

## Statistical Calibration

Statistical methods can involve finding **point estimates** or **distributions** of parameters (more on this later).

## Formalizing Calibration

Let $F(\mathbf{x}; \theta)$ be the simulation model function:

- $\mathbf{x}$ are the "control variables," which are external forcings or variables affecting the simulation setting;
- $\theta$ are the "calibration variables" or "parameters."

Then the goal of calibration is to identify parameters $\theta$ which minimize the mismatch between $F(\mathbf{x}; \theta)$ and data $\mathbf{y}$.

## Data-Model Discrepancy

As mentioned, all models will necessarily be wrong. We formalize this idea with the notion of **discrepancy**.

We can write the data as:

$$\mathbf{y} = z(\mathbf{x}) + \varepsilon,$$

- $z(\mathbf{x})$ is the "true" system state at control variable $\mathbf{x}$;
- $\varepsilon$ are observation errors.

## Data-Model Discrepancy

Then the **discrepancy** $\zeta$ between the model simulation and the system is:

$$\zeta(\mathbf{x}; \theta) = z(\mathbf{x}) - F(\mathbf{x}; \theta).$$

::: {.fragment .fade-in}
Then the observations can be written as:

$$
\mathbf{y} = \underbrace{F(\mathbf{x}; \theta)}_{\text{model output}} + \underbrace{\zeta(\mathbf{x}; \theta)}_{\text{discrepancy}} + \underbrace{\varepsilon}_{\text{error}}
$$
:::

## Why is Hand-Tuning Problematic?

This representation shows where hand-tuning can fall short:

::: {.incremental}
1. Assumptions about the discrepancy and error are not made explicit;
2. Assumptions about discrepancy and error may not be applied consistently;
3. The errors in the data collection process may be large, and neglecting discrepancy and error may result in overfitting to just one realization.
:::

As a result, we will focus on statistical calibration.

## Model Calibration and Discrepancy

Statistical calibration then involves specifying a probability model for the data, which can include:

- Distributions/ranges for the calibration parameters;
- A probability model for the discrepancy and observation errors.

Then the goal is to find parameters which maximize the *likelihood* (probability) of the data.

## Example: Mean-Squared Error

Consider finding parameters $\theta$ which minimize mean-squared error (MSE):

$$
\text{MSE} = \sum_{i=1}^n \left(y_i - F(x_i; \theta)\right)^2.
$$

This is an *extremely* common metric used for model calibration. Where does it come from?

## Example: Mean-Squared Error

It turns out this is the same thing as finding the parameters which maximize likelihood under the assumption of normally-distributed errors and discrepancy. 

Assume that the errors are given by $\mathcal{N}(0, \sigma^2)$. Then the likelihood of the data given the simulation model is:

$$
\mathcal{L}(\theta, \mathbf{y}; F) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp(-\frac{y_i - F(x_i; \theta)^2}{2\sigma^2})
$$

## Example: Mean-Squared Error

To maximize this likelihood, let's take the logarithm:

$$
\begin{align}
\log \mathcal{L}(\theta, \mathbf{y}; F) &= \sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2}(y_i - F(x_i; \theta))  ^2 \right] \\
&= n \log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - F(x_i; \theta))^2
\end{align}
$$

## Example: Mean-Squared Error

If we ignore the constants (including the assumed error standard deviation $\sigma$), we're left with wanting to maximize

$$-\sum_{i=1}^n (y_i - F(x_i; \theta))^2.$$

::: {.fragment .fade-in}
Maximizing this is equivalent to minimizing the negative:

$$
\sum_{i=1}^n (y_i - F(x_i; \theta))^2 = \text{MSE}
$$
:::

## More Complicated Discrepancy-Error Structures

We can make similar estimatesfor more complex discrepancy structures (such as assuming normally-distributed errors and autoregressive discrepancy), but they may not result in "standard" error metrics. 



# Model Validation

## What is Model Validation?

**Key Question**: How do we know that our model is behaving as expected?

::: {.fragment .fade-in}
Won't spend too much time on this, but can use a combination of graphical checks and quantitative metrics ($R^2$, RMSE).

:::

## Connection Between Validation and Calibration

Model validation is often conditional on the calibration.

Have to look at model output from a particular parameter vector (or set of parameter vectors).

## Residual Checking

Check üëè Your üëè Residuals!

Are they consistent with the assumptions made when building your probability model for the data?

::: {.fragment .fade-in}
**For example**:

We calibrated our model with MSE. Are our residuals normally-distributed?
:::

# Example: Modeling Sea-Level Rise

## Mathematical Model

[Rahmstorf (2007)](https://doi.org/10.1073/pnas.0907765106) proposed the following semi-empirical model for sea-level $H$ response to changes to global mean temperature $T$:

$$\frac{dH}{dt} = \alpha (T - T_0)$$

- $T_0$ is the temperature ($^\circ C$) where sea-level is in equilibrium ($dH/dt = 0$)
- $\alpha$ is the sea-level rise sensitivity to temperature.

## Plotting the Data

```{julia}
#| layout-nrow: 2

# load data files
slr_data = DataFrame(load("data/CSIRO_Recons_gmsl_yr_2015.csv"))
gmt_data = DataFrame(load("data/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv"))
slr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times
dat = leftjoin(slr_data, gmt_data, on="Time") # join data frames on time
select!(dat, [1, 2, 4])  # drop columns we don't need

p1 = scatter(dat[:, 1], dat[:, 3], color=:black, legend=:false, grid=false, xaxis="Year", yaxis="Temperature \n Anomaly (¬∞C)", guidefontsize=16, tickfontsize=15, left_margin=10mm, bottom_margin=10mm)
plot!(p1, size=(1200, 300))
display(p1)

p2 = scatter(dat[:, 1], dat[:, 2], color=:black, legend=:false, grid=false, xaxis="Year", yaxis="Sea Level \n Anomaly (mm)", guidefontsize=16, tickfontsize=15, left_margin=10mm, bottom_margin=10mm)
plot!(p2, size=(1200, 300))
display(p2)
```

## Model Calibration

Let's calibrate this model, assuming normal discrepancy (so minimize MSE). First, we need to discretize it:

::: {.fragment .fade-in}
$$
\begin{gather}
\frac{dH}{dt} = \alpha (T - T_0) \\[1em]
\frac{H(t+1) - H(t)}{\Delta t} = \alpha (T(t) - T_0) \\[1em]
H(t+1) = \alpha (T(t) - T_0) \Delta t + H(t)
\end{gather}
$$
:::

## Calibration Parameter Selection

This means we have three calibration parameters: 

- $\alpha$, 
- $T_0$, 
- $H(0)$ (initial condition).

## Calibration Results

```{julia}
#| output: false

function rahmstorf_model(Œ±, T‚ÇÄ, H‚ÇÄ, temp_data)
    temp_effect = Œ± .* (temp_data .- T‚ÇÄ)
    slr_predict = cumsum(temp_effect) .+ H‚ÇÄ
    return slr_predict
end

mse(y, yÃÇ) = mean((y .- yÃÇ).^2)

years = dat[:, 1]
sealevels = dat[:, 2]
temp = dat[:, 3]

function minimize_mse(p, temp, sealevels)
    predict = rahmstorf_model(p[1], p[2], p[3], temp)
    return mse(predict, sealevels)
end
optim_out = Optim.optimize(p -> minimize_mse(p, temp, sealevels), [1.0, 0.0, 0.0])
params = Optim.minimizer(optim_out)
```

:::: {.columns}
::: {.column width=40%}
Optimizing, we get:

- $\alpha \approx 1.86 \text{mm} / ^\circ C$
- $T_0 \approx -0.97 ^\circ C$
- $H(0) \approx -157 \text{mm}$.
:::
::: {.column width=60%}
::: {.fragment .fade-in}
```{julia}
slr_hindcast = rahmstorf_model(params[1], params[2], params[3], temp)
scatter(years, sealevels, color=:black, alpha=0.7, label="Reconstructed Data", legend=:topleft, grid=false, xaxis="Year", yaxis="Sea Level Anomaly (mm)", guidefontsize=16, tickfontsize=15, legendfontsize=16, left_margin=10mm, bottom_margin=5mm)
plot!(years, slr_hindcast, color=:red, linewidth=3, label="Calibrated Model Output")
plot!(size=(750, 600))
```
:::
:::
::::

## Visualizing Residuals

```{julia}
residuals = sealevels .- slr_hindcast
scatter(years, residuals, legend=:false, xlabel="Year", ylabel="Model Residual (mm)", markersize=5, grid=:false, guidefontsize=16, tickfontsize=15, bottom_margin=5mm, left_margin=10mm)
plot!(size=(1200, 600))
```

## Are The Residuals Normal And Independent?

```{julia}
#| layout-ncol: 2

p1 = histogram(residuals, tickfontsize=15, guidefontsize=16, titlefontsize=16, labelfontsize=16, legendfontsize=14, legend=:false, xlabel="Residual (mm)", ylabel="Count")
plot!(p1, size=(600, 500))
display(p1)

resid_pacf = pacf(residuals, 1:5)
p2 = scatter(resid_pacf, linetype=:stem, markersize=5, marker=:circle, markerstrokewidth=3, markeralpha=0.6, color=:red, xlabel="Lag", ylabel="Partial Autocorrelation", tickfontsize=15, guidefontsize=16, titlefontsize=16, labelfontsize=16, legendfontsize=14, legend=:false)
plot!(p2, size=(600, 500))
display(p2)
```

## Accounting for Autocorrelation

Let's say we want to account for the autocorrelation in the residuals. 

Where is this likely to enter into our discrepancy-error decomposition?

## Autocorrelated Discrepancy

Then our probability model might look like this:

$$
\begin{align}
y_t &= F(x_t; \theta) + \zeta_t + \varepsilon_t \\[1.0em]
\zeta_t &\sim \rho \zeta_{t-1} + \omega_t \\[1.0em]
\varepsilon_t &\sim \mathcal{N}(0, \sigma_\varepsilon^2)\\[1.0em]
\omega_t &\sim \mathcal{N}(0, \sigma_\omega^2)
\end{align}
$$

## Autocorrelated Discrepancy

We won't go more deeply into this here -- this doesn't result in a "nice" error metric like the MSE. 

But we could write out the probability distribution associated with this probability model (like we did with the MSE) and maximize, possibly treating the statistical parameters $\rho, \sigma_\varepsilon^2, \sigma_\omega^2$ as uncertain as well.

# Next Steps: Uncertainty Analysis

## Uncertainty Analysis

Everything that we talked about so far involve **point estimates**: we get single values of parameters which give us a "best fit" to the provided data.

When could this be a problem?

::: {.fragment .fade-in}
- **What if our data is itself uncertain?**
- **What if multiple parameters are roughly equally consistent with the data**?
:::

## Uncertainty Analysis

To address these questions, we can quantify or characterize relevant uncertainties.

Many methods, lots to do and learn here.

# Questions?

## Thanks!

If you'd like to follow up:

- Lab Website: <https://viveks.bee.cornell.edu>
- Email: [viveks@cornell.edu](mailto:viveks@cornell.edu)